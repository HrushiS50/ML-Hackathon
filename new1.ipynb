{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\Hrushikesh\\OneDrive\\Desktop\\ML Hackathon\\student_resource 3\\dataset\\mapped_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_value</th>\n",
       "      <th>Image</th>\n",
       "      <th>Image_Arrays</th>\n",
       "      <th>OCR_Text</th>\n",
       "      <th>entity_number</th>\n",
       "      <th>entity_name_item_volume</th>\n",
       "      <th>entity_name_item_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>entity_unit_cup</th>\n",
       "      <th>entity_unit_gallon</th>\n",
       "      <th>entity_unit_gram</th>\n",
       "      <th>entity_unit_kilogram</th>\n",
       "      <th>entity_unit_milligram</th>\n",
       "      <th>entity_unit_millilitre</th>\n",
       "      <th>entity_unit_ounce</th>\n",
       "      <th>entity_unit_pound</th>\n",
       "      <th>entity_unit_volt</th>\n",
       "      <th>entity_unit_watt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n",
       "      <td>748919</td>\n",
       "      <td>500.0 gram</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=1600x1600...</td>\n",
       "      <td>[[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...</td>\n",
       "      <td>100% NATUR\\n\\n</td>\n",
       "      <td>500.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n",
       "      <td>916768</td>\n",
       "      <td>1.0 cup</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=1200x1200...</td>\n",
       "      <td>[[[155 190 209]\\n  [153 188 207]\\n  [153 188 2...</td>\n",
       "      <td>GEPRAGTES\\n\\nDesigned in\\n\\nLIZENZIERTE UND GE...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n",
       "      <td>459516</td>\n",
       "      <td>0.709 gram</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=1081x1081...</td>\n",
       "      <td>[[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...</td>\n",
       "      <td>Serving Size: 1 Tablet (0.709 g) | Each servin...</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n",
       "      <td>459516</td>\n",
       "      <td>0.709 gram</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=1081x1081...</td>\n",
       "      <td>[[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n",
       "      <td>731432</td>\n",
       "      <td>1400 milligram</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=1500x1500...</td>\n",
       "      <td>[[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...</td>\n",
       "      <td>HIGH STRENGTH\\n\\nPSYLLIUM\\n\\nHUSK\\n\\n</td>\n",
       "      <td>1400.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71XK5d3Oh9...</td>\n",
       "      <td>416664</td>\n",
       "      <td>49.0 watt</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=2560x2560...</td>\n",
       "      <td>[[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61kyBEJYDe...</td>\n",
       "      <td>459516</td>\n",
       "      <td>500 milligram</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=1500x1500...</td>\n",
       "      <td>[[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71uQmsTESv...</td>\n",
       "      <td>459516</td>\n",
       "      <td>500 milligram</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=1500x1500...</td>\n",
       "      <td>[[[253 253 253]\\n  [254 254 254]\\n  [254 254 2...</td>\n",
       "      <td>GREEN COFFEE\\n\\nDiscover weliness\\n\\nS42\\n6)\\n...</td>\n",
       "      <td>500.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71jG8BOi4W...</td>\n",
       "      <td>241608</td>\n",
       "      <td>16.0 gram</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=1600x1600...</td>\n",
       "      <td>[[[227 228 230]\\n  [228 229 231]\\n  [228 229 2...</td>\n",
       "      <td>16G Thick High Grade 304 Stainless Steel\\n\\n— ...</td>\n",
       "      <td>16.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61390hosjF...</td>\n",
       "      <td>308856</td>\n",
       "      <td>8 kilogram</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=984x1000 ...</td>\n",
       "      <td>[[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...</td>\n",
       "      <td>SEPURINA?\\n\\n| » Bae PUPPYI¥E.22) §\\neS\\n\\n</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                         image_link  group_id  \\\n",
       "0            0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919   \n",
       "1            1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768   \n",
       "2            2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516   \n",
       "3            3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516   \n",
       "4            4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432   \n",
       "..         ...                                                ...       ...   \n",
       "95          95  https://m.media-amazon.com/images/I/71XK5d3Oh9...    416664   \n",
       "96          96  https://m.media-amazon.com/images/I/61kyBEJYDe...    459516   \n",
       "97          97  https://m.media-amazon.com/images/I/71uQmsTESv...    459516   \n",
       "98          98  https://m.media-amazon.com/images/I/71jG8BOi4W...    241608   \n",
       "99          99  https://m.media-amazon.com/images/I/61390hosjF...    308856   \n",
       "\n",
       "      entity_value                                              Image  \\\n",
       "0       500.0 gram  <PIL.Image.Image image mode=RGB size=1600x1600...   \n",
       "1          1.0 cup  <PIL.Image.Image image mode=RGB size=1200x1200...   \n",
       "2       0.709 gram  <PIL.Image.Image image mode=RGB size=1081x1081...   \n",
       "3       0.709 gram  <PIL.Image.Image image mode=RGB size=1081x1081...   \n",
       "4   1400 milligram  <PIL.Image.Image image mode=RGB size=1500x1500...   \n",
       "..             ...                                                ...   \n",
       "95       49.0 watt  <PIL.Image.Image image mode=RGB size=2560x2560...   \n",
       "96   500 milligram  <PIL.Image.Image image mode=RGB size=1500x1500...   \n",
       "97   500 milligram  <PIL.Image.Image image mode=RGB size=1500x1500...   \n",
       "98       16.0 gram  <PIL.Image.Image image mode=RGB size=1600x1600...   \n",
       "99      8 kilogram  <PIL.Image.Image image mode=RGB size=984x1000 ...   \n",
       "\n",
       "                                         Image_Arrays  \\\n",
       "0   [[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...   \n",
       "1   [[[155 190 209]\\n  [153 188 207]\\n  [153 188 2...   \n",
       "2   [[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...   \n",
       "3   [[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...   \n",
       "4   [[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...   \n",
       "..                                                ...   \n",
       "95  [[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...   \n",
       "96  [[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...   \n",
       "97  [[[253 253 253]\\n  [254 254 254]\\n  [254 254 2...   \n",
       "98  [[[227 228 230]\\n  [228 229 231]\\n  [228 229 2...   \n",
       "99  [[[255 255 255]\\n  [255 255 255]\\n  [255 255 2...   \n",
       "\n",
       "                                             OCR_Text  entity_number  \\\n",
       "0                                      100% NATUR\\n\\n        500.000   \n",
       "1   GEPRAGTES\\n\\nDesigned in\\n\\nLIZENZIERTE UND GE...          1.000   \n",
       "2   Serving Size: 1 Tablet (0.709 g) | Each servin...          0.709   \n",
       "3                                                 NaN          0.709   \n",
       "4               HIGH STRENGTH\\n\\nPSYLLIUM\\n\\nHUSK\\n\\n       1400.000   \n",
       "..                                                ...            ...   \n",
       "95                                                NaN         49.000   \n",
       "96                                                NaN        500.000   \n",
       "97  GREEN COFFEE\\n\\nDiscover weliness\\n\\nS42\\n6)\\n...        500.000   \n",
       "98  16G Thick High Grade 304 Stainless Steel\\n\\n— ...         16.000   \n",
       "99        SEPURINA?\\n\\n| » Bae PUPPYI¥E.22) §\\neS\\n\\n          8.000   \n",
       "\n",
       "    entity_name_item_volume  entity_name_item_weight  ...  entity_unit_cup  \\\n",
       "0                       0.0                      1.0  ...              0.0   \n",
       "1                       1.0                      0.0  ...              1.0   \n",
       "2                       0.0                      1.0  ...              0.0   \n",
       "3                       0.0                      1.0  ...              0.0   \n",
       "4                       0.0                      1.0  ...              0.0   \n",
       "..                      ...                      ...  ...              ...   \n",
       "95                      0.0                      0.0  ...              0.0   \n",
       "96                      0.0                      1.0  ...              0.0   \n",
       "97                      0.0                      1.0  ...              0.0   \n",
       "98                      0.0                      1.0  ...              0.0   \n",
       "99                      0.0                      1.0  ...              0.0   \n",
       "\n",
       "    entity_unit_gallon  entity_unit_gram  entity_unit_kilogram  \\\n",
       "0                  0.0               1.0                   0.0   \n",
       "1                  0.0               0.0                   0.0   \n",
       "2                  0.0               1.0                   0.0   \n",
       "3                  0.0               1.0                   0.0   \n",
       "4                  0.0               0.0                   0.0   \n",
       "..                 ...               ...                   ...   \n",
       "95                 0.0               0.0                   0.0   \n",
       "96                 0.0               0.0                   0.0   \n",
       "97                 0.0               0.0                   0.0   \n",
       "98                 0.0               1.0                   0.0   \n",
       "99                 0.0               0.0                   1.0   \n",
       "\n",
       "    entity_unit_milligram  entity_unit_millilitre  entity_unit_ounce  \\\n",
       "0                     0.0                     0.0                0.0   \n",
       "1                     0.0                     0.0                0.0   \n",
       "2                     0.0                     0.0                0.0   \n",
       "3                     0.0                     0.0                0.0   \n",
       "4                     1.0                     0.0                0.0   \n",
       "..                    ...                     ...                ...   \n",
       "95                    0.0                     0.0                0.0   \n",
       "96                    1.0                     0.0                0.0   \n",
       "97                    1.0                     0.0                0.0   \n",
       "98                    0.0                     0.0                0.0   \n",
       "99                    0.0                     0.0                0.0   \n",
       "\n",
       "    entity_unit_pound  entity_unit_volt  entity_unit_watt  \n",
       "0                 0.0               0.0               0.0  \n",
       "1                 0.0               0.0               0.0  \n",
       "2                 0.0               0.0               0.0  \n",
       "3                 0.0               0.0               0.0  \n",
       "4                 0.0               0.0               0.0  \n",
       "..                ...               ...               ...  \n",
       "95                0.0               0.0               1.0  \n",
       "96                0.0               0.0               0.0  \n",
       "97                0.0               0.0               0.0  \n",
       "98                0.0               0.0               0.0  \n",
       "99                0.0               0.0               0.0  \n",
       "\n",
       "[100 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image file downloaded_images_train/61I9XdN6OFL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71gSRbyXmoL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61BZ4zrjZXL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/612mrlqiI4L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/617Tl40LOXL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61QsBSE7jgL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81xsq6vf2qL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71DiLRHeZdL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/91Cma3RzseL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71jBLhmTNlL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81N73b5khVL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61oMj2iXOuL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/91LPf6OjV9L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81fOxWWWKYL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81dzao1Ob4L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/91-iahVGEDL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81S2+GnYpTL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81e2YtCOKvL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81RNsNEM1EL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/91prZeizZnL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/31EvJszFVfL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61wzlucTREL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61sQ+qAKr4L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81x77l2T5NL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71nywfWZUwL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71nywfWZUwL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/51WsuKKAVrL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61XGDKap+JL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/715vVcWJxGL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/613v+2W4UwL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71+fn9TWQmL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71aKgRRQ2wL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71rKXZJrh4L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71D824lbRvL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71004c9tzfL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/51bQPPtMqYL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61o2ntPNNgL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61o2ntPNNgL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71IUuTJ8QwL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/915JHkwtcrL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71cjrYndwIL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81hnk2WXO3L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61HXgujoxpL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/613G8GOyLSL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71YyZ2iPyZL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81K3JwUCnQL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/41wvffSxB4L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/91cErO-KbLL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/817vo3DcCNL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61AHQ35poHL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61WFh8RCQYL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/711SATIDrmL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61x6RSjwQIL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/613BeFNwHcL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61hWZdkq6WL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71E7CU55dcL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61c+hSNnnZL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/915w0BdW-gL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61sx0ezNNLL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71ldprwbKrL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71E9iF-bmKL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71sWRp1SNwL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61Fwq4GeTmL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61-oj+N+BxL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71e6kJLE+LL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71SuzaRS7gL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71nsfFCXF0L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71hgN7yu9OL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61SmT8pkLtL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71ZtDgGX+iL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/413FQB0ZMLL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/41EjbFu-+yL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71dWDwMhWmS.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61d6Kj80QSL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71bvOuz9w1L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71l0M0tMGjL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71Lpqdrpi4L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71jLIbCcwOL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/718EdwGgyVL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/713twQgCHSL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61I0O1qJbhS.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61eOO5IW4NL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/716AQpAJjZL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71FVeRd2jqL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81njuNSPdjL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/51xfRlxWIXL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71duwM3SjpL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/612xIhPMHqL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/51b9JEHOriL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/81lgxfKqUUL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/814sAvV89SL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61cMeogK8gL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/811VfR10yxL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71WLYfmMqQL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61Dq3LRei9L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71XK5d3Oh9L.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61kyBEJYDeL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71uQmsTESvL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/71jG8BOi4WL.jpg does not exist. Skipping...\n",
      "Warning: Image file downloaded_images_train/61390hosjFL.jpg does not exist. Skipping...\n",
      "Total images mapped: 100\n",
      "Successfully loaded images: 0\n",
      "Skipped images: 100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m mapped_train_dataset \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mhead(MAX_IMAGES)\n\u001b[0;32m    107\u001b[0m mapped_train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_images\n\u001b[1;32m--> 108\u001b[0m mapped_train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage_Arrays\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_arrays\n\u001b[0;32m    109\u001b[0m mapped_train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOCR_Text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ocr_texts\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Step 1: Split 'entity_value' into numeric value and unit\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hrushikesh\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4091\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4090\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\Hrushikesh\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4300\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4292\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4293\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4300\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   4302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4303\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4304\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4305\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4306\u001b[0m     ):\n\u001b[0;32m   4307\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\Hrushikesh\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5039\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5039\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hrushikesh\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (0) does not match length of index (100)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "\n",
    "# Set up pytesseract\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess and resize image\n",
    "def process_image(image, size=(244, 244)):\n",
    "    try:\n",
    "        # Resize the image\n",
    "        image_resized = image.resize(size)\n",
    "        # Convert to numpy array\n",
    "        image_array = np.array(image_resized)\n",
    "        return image_array\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# OCR function to extract text from the image\n",
    "def extract_text_from_image(image):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Function to load images and apply OCR\n",
    "def load_images_with_ocr(df, image_dir, max_images, batch_size=100):\n",
    "    total_images = len(df)\n",
    "    batches = (total_images + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "    images = []\n",
    "    image_arrays = []\n",
    "    all_images = []\n",
    "    ocr_texts = []  # To store OCR results\n",
    "    df = df.head(max_images)\n",
    "\n",
    "    for batch_index in range(batches):\n",
    "        batch_df = df.iloc[batch_index * batch_size:(batch_index + 1) * batch_size]\n",
    "        successfully_loaded = 0\n",
    "        skipped_files = 0\n",
    "        total_mapped = 0\n",
    "\n",
    "        for index, row in batch_df.iterrows():\n",
    "            total_mapped += 1\n",
    "            # Extract the filename from the image link\n",
    "            image_filename = row['image_link'].split('/')[-1]\n",
    "            image_path = os.path.join(image_dir, image_filename)\n",
    "            image_path = image_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "            # Load the image if it exists, else append None\n",
    "            if os.path.exists(image_path):\n",
    "                try:\n",
    "                    with Image.open(image_path) as img:\n",
    "                        # Resize image\n",
    "                        image_array = process_image(img)\n",
    "                        image_arrays.append(image_array)\n",
    "\n",
    "                        # Apply OCR to extract text\n",
    "                        ocr_text = extract_text_from_image(img)\n",
    "                        ocr_texts.append(ocr_text)\n",
    "\n",
    "                        images.append(img.copy())  # Copy the image to avoid keeping the file open\n",
    "                        successfully_loaded += 1\n",
    "                except (UnidentifiedImageError, OSError) as e:\n",
    "                    print(f\"Warning: Error opening image file {image_path}: {e}\")\n",
    "                    images.append(None)  # Append None if the image is invalid or corrupted\n",
    "                    ocr_texts.append(\"\")  # Append empty string for OCR result if error\n",
    "                    skipped_files += 1\n",
    "            else:\n",
    "\n",
    "                print(f\"Warning: Image file {image_path} does not exist. Skipping...\")\n",
    "                images.append(None)\n",
    "                ocr_texts.append(\"\")  # No OCR text for missing images\n",
    "                skipped_files += 1\n",
    "        all_images.extend(images)\n",
    "        print(f\"Total images mapped: {total_mapped}\")\n",
    "        print(f\"Successfully loaded images: {successfully_loaded}\")\n",
    "        print(f\"Skipped images: {skipped_files}\")\n",
    "\n",
    "    return images, image_arrays, ocr_texts\n",
    "\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(r'C:\\Users\\Hrushikesh\\OneDrive\\Desktop\\ML Hackathon\\student_resource 3\\dataset\\mapped_train.csv')\n",
    "\n",
    "train_image_dir = 'downloaded_images_train'\n",
    "MAX_IMAGES = 100\n",
    "\n",
    "# Load training images and OCR text\n",
    "train_images, train_arrays, ocr_texts = load_images_with_ocr(train_df, train_image_dir, MAX_IMAGES, 100)\n",
    "\n",
    "# Add OCR text to the dataset\n",
    "mapped_train_dataset = train_df.head(MAX_IMAGES)\n",
    "mapped_train_dataset['Image'] = train_images\n",
    "mapped_train_dataset['Image_Arrays'] = train_arrays\n",
    "mapped_train_dataset['OCR_Text'] = ocr_texts\n",
    "\n",
    "# Step 1: Split 'entity_value' into numeric value and unit\n",
    "mapped_train_dataset[['entity_number', 'entity_unit']] = mapped_train_dataset['entity_value'].str.extract(r'([0-9.]+)\\s*(\\w+)')\n",
    "\n",
    "# Step 2: Convert the numeric part to float\n",
    "mapped_train_dataset['entity_number'] = mapped_train_dataset['entity_number'].astype(float)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(mapped_train_dataset[['OCR_Text','entity_value', 'entity_number', 'entity_unit']])\n",
    "\n",
    "# # Step 2: Convert the numeric part to float\n",
    "# mapped_train_dataset['entity_number'] = pd.to_numeric(mapped_train_dataset['entity_number'], errors='coerce')\n",
    "\n",
    "# # Display the resulting dataframe with OCR text, numeric value, and units\n",
    "# print(mapped_train_dataset[['OCR_Text', 'entity_number', 'entity_unit']])\n",
    "\n",
    "# # Drop the 'entity_value' column (if exists) and handle the rest of the dataset\n",
    "# mapped_train_dataset = mapped_train_dataset.drop(columns='entity_value', errors='ignore')\n",
    "\n",
    "# One-hot encode the 'entity_name' and 'entity_unit' columns\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Encode entity_name\n",
    "categorical_column = mapped_train_dataset[['entity_name']]\n",
    "column_encoded = encoder.fit_transform(categorical_column)\n",
    "column_encoded_df = pd.DataFrame(column_encoded.toarray(), columns=encoder.get_feature_names_out(['entity_name']))\n",
    "mapped_train_dataset = pd.concat([mapped_train_dataset.drop(columns=['entity_name']), column_encoded_df], axis=1)\n",
    "\n",
    "# Encode entity_unit\n",
    "categorical_column = mapped_train_dataset[['entity_unit']]\n",
    "column_encoded = encoder.fit_transform(categorical_column)\n",
    "column_encoded_df = pd.DataFrame(column_encoded.toarray(), columns=encoder.get_feature_names_out(['entity_unit']))\n",
    "mapped_train_dataset = pd.concat([mapped_train_dataset.drop(columns=['entity_unit']), column_encoded_df], axis=1)\n",
    "\n",
    "\n",
    "# Convert train_arrays (list) to a NumPy array\n",
    "train_arrays = np.array(train_arrays)\n",
    "# Flatten the image arrays\n",
    "train_image_arrays_flat = train_arrays.reshape(train_arrays.shape[0], -1)\n",
    "\n",
    "col_to_drop=['image_link','group_id','entity_value','Image','OCR_Text']\n",
    "\n",
    "print(mapped_train_dataset.columns.tolist())\n",
    "\n",
    "\n",
    "# Drop the columns from the DataFrame\n",
    "mapped_train_dataset_dropped = mapped_train_dataset.drop(columns=col_to_drop, errors='ignore')\n",
    "\n",
    "# Print the columns to ensure correct columns are dropped\n",
    "print(\"Columns after dropping:\")\n",
    "print(mapped_train_dataset_dropped.columns.tolist())\n",
    "\n",
    "# Combine image arrays and encoded categorical data\n",
    "try:\n",
    "    train_features = np.hstack((train_image_arrays_flat, mapped_train_dataset_dropped.to_numpy()))\n",
    "    print(\"Shape of train_features:\", train_features.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error combining features: {e}\")\n",
    "\n",
    "# Extract target variable\n",
    "target_number = mapped_train_dataset_dropped['entity_number']\n",
    "target_unit = mapped_train_dataset_dropped.filter(regex='^entity_unit_')\n",
    "\n",
    "# Ensure all data is numeric\n",
    "mapped_train_dataset_dropped = mapped_train_dataset_dropped.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert train_arrays (list) to a NumPy array\n",
    "train_arrays = np.array(train_arrays)\n",
    "\n",
    "# Flatten the image arrays\n",
    "train_image_arrays_flat = train_arrays.reshape(train_arrays.shape[0], -1)\n",
    "\n",
    "# Drop the columns from the DataFrame\n",
    "mapped_train_dataset_dropped = mapped_train_dataset_dropped.drop(columns=col_to_drop, errors='ignore')\n",
    "\n",
    "# Combine image arrays and encoded categorical data\n",
    "try:\n",
    "    train_features = np.hstack((train_image_arrays_flat, mapped_train_dataset_dropped.to_numpy()))\n",
    "    print(\"Shape of train_features:\", train_features.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error combining features: {e}\")\n",
    "\n",
    "# Check for NaNs and Infs\n",
    "print(\"NaNs in train_features:\", np.any(np.isnan(train_features)))\n",
    "print(\"Infs in train_features:\", np.any(np.isinf(train_features)))\n",
    "\n",
    "# Replace NaNs with a specific value (e.g., 0)\n",
    "train_features = np.nan_to_num(train_features)\n",
    "\n",
    "# Proceed with train/test split and model fitting\n",
    "X_train, X_valid, y_train_number, y_valid_number = train_test_split(train_features, target_number, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train_unit, y_valid_unit = train_test_split(train_features, target_unit, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train_number)\n",
    "y_pred_number = regressor.predict(X_valid)\n",
    "print(\"Mean Squared Error for entity_number:\", mean_squared_error(y_valid_number, y_pred_number))\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train_unit)\n",
    "y_pred_unit = classifier.predict(X_valid)\n",
    "print(\"Accuracy for entity_unit:\", accuracy_score(y_valid_unit, y_pred_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
